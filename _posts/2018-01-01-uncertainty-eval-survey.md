---
layout: paper
category: paper
title:  "In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation"
authors: "Jessica Hullman, Xiaoli Qiao, Michael Correll, Alex Kale, Matthew Kay"
venue: "IEEE Trans. Visualization & Comp. Graphics (Proc. INFOVIS) 2018"
thumb: "assets/images/paper-thumb-uncertainty-eval-survey.png"
banner: "assets/images/paper-banner-uncertainty-eval-survey.png"
caption: "A collection of 372 evaluation paths observed across a sample of 86 publications with uncertainty visualization evaluations."
pdf: "assets/papers/2018-uncertainty-eval-survey.pdf"
addtinoals:
  - supplement: "https://github.com/jhullman/uncertaintyVisEval"
  - website: "https://visualization.ischool.uw.edu/~xiaoliq/uncertainty_vis_eval/index.html"
---

<!-- abstract -->
Understanding and accounting for uncertainty is critical to effectively reasoning about visualized data. However, evaluating the impact of an uncertainty visualization is complex due to the difficulties that people have interpreting uncertainty and the challenge of defining correct behavior with uncertainty information. Currently, evaluators of uncertainty visualization must rely on general purpose visualization evaluation frameworks which can be ill-equipped to provide guidance with the unique difficulties of assessing judgments under uncertainty. To help evaluators navigate these complexities, we present a taxonomy for characterizing decisions made in designing an evaluation of an uncertainty visualization. Our taxonomy differentiates six levels of decisions that comprise an uncertainty visualization evaluation: the behavioral targets of the study, expected effects from an uncertainty visualization, evaluation goals, measures, elicitation techniques, and analysis approaches. Applying our taxonomy to 86 user studies of uncertainty visualizations, we find that existing evaluation practice, particularly in visualization research, focuses on Performance and Satisfaction-based measures that assume more predictable and statistically-driven judgment behavior than is suggested by research on human judgment and decision making. We reflect on common themes in evaluation practice concerning the interpretation and semantics of uncertainty, the use of confidence reporting, and a bias toward evaluating performance as accuracy rather than decision quality. We conclude with a concrete set of recommendations for evaluators designed to reduce the mismatch between the conceptualization of uncertainty in visualization versus other fields.